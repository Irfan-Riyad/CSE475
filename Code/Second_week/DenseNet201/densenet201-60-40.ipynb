{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13329590,"sourceType":"datasetVersion","datasetId":8450902,"isSourceIdPinned":false}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# CELL 1: SETUP AND IMPORTS\n# ==============================================================================\nimport os\nimport shutil\nimport random\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time # <-- ADD THIS\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n# --- ADD THESE IMPORTS for ROC/AUC ---\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom scipy.special import softmax\n# ------------------------------------\n\nimport kagglehub\n\nprint(\"All libraries imported successfully.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T05:24:52.619762Z","iopub.execute_input":"2025-10-27T05:24:52.620466Z","iopub.status.idle":"2025-10-27T05:25:06.856058Z","shell.execute_reply.started":"2025-10-27T05:24:52.620435Z","shell.execute_reply":"2025-10-27T05:25:06.855188Z"}},"outputs":[{"name":"stdout","text":"All libraries imported successfully.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: DOWNLOAD AND PREPARE DATASET\n# ==============================================================================\n# --- Download from Kaggle ---\nimport kagglehub\npath = kagglehub.dataset_download(\"mdriyadhossain/ai-medleafx\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T05:25:21.906278Z","iopub.execute_input":"2025-10-27T05:25:21.906561Z","iopub.status.idle":"2025-10-27T05:25:22.043889Z","shell.execute_reply.started":"2025-10-27T05:25:21.906538Z","shell.execute_reply":"2025-10-27T05:25:22.043272Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/ai-medleafx\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/ai-medleafx\"\nprint(\"Verified path to dataset files:\", dataset_path)\n\n# Assuming the structure is dataset_path -> 'Resized Image' -> Plant Type -> Disease/Health Status -> Images\nresized_image_base_path = os.path.join(dataset_path, 'Resized Image')\n\nall_class_dirs = []\nclass_counts = {}\n\nprint(\"\\nExploring dataset structure and counting images:\")\n\ntry:\n    # List plant type directories within 'Resized Image'\n    plant_type_dirs = [d for d in os.listdir(resized_image_base_path) if os.path.isdir(os.path.join(resized_image_base_path, d))]\n    print(f\"\\nFound {len(plant_type_dirs)} plant types:\", plant_type_dirs)\n\n    if not plant_type_dirs:\n        print(\"No plant type directories found within 'Resized Image'. Cannot proceed with expected structure.\")\n    else:\n        # Iterate through each plant type directory\n        for plant_type_dir in plant_type_dirs:\n            plant_type_path = os.path.join(resized_image_base_path, plant_type_dir)\n\n            # List disease/health status directories within each plant type\n            status_dirs = [d for d in os.listdir(plant_type_path) if os.path.isdir(os.path.join(plant_type_path, d))]\n            print(f\"\\nExploring '{plant_type_dir}': Found {len(status_dirs)} status categories:\", status_dirs)\n\n            if not status_dirs:\n                print(f\"No status directories found within '{plant_type_dir}'.\")\n            else:\n                # Iterate through each status directory (these are the actual classes)\n                for status_dir in status_dirs:\n                    class_name = f\"{plant_type_dir}_{status_dir}\" # Combine plant type and status for a unique class name\n                    class_path = os.path.join(plant_type_path, status_dir)\n                    all_class_dirs.append(class_name)\n\n                    # Count image files in the class directory\n                    image_files_in_class = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg','.JPG'))]\n                    class_counts[class_name] = len(image_files_in_class)\n                    print(f\"- {class_name}: {class_counts[class_name]} images\")\n\n        # Determine the total number of classes\n        num_classes = len(all_class_dirs)\n        print(f\"\\nTotal number of classes: {num_classes}\")\n\n        # Display class distribution summary\n        print(\"\\nClass distribution:\")\n        for class_name, count in class_counts.items():\n            print(f\"- {class_name}: {count}\")\n\n        # Calculate total number of images\n        total_images = sum(class_counts.values())\n        print(f\"\\nTotal number of images: {total_images}\")\n\nexcept FileNotFoundError:\n    print(f\"Error: The directory '{resized_image_base_path}' was not found.\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n\n\nprint(\"\\nDataset exploration and counting complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T05:25:33.865372Z","iopub.execute_input":"2025-10-27T05:25:33.866092Z","iopub.status.idle":"2025-10-27T05:25:38.807326Z","shell.execute_reply.started":"2025-10-27T05:25:33.866060Z","shell.execute_reply":"2025-10-27T05:25:38.806735Z"}},"outputs":[{"name":"stdout","text":"Verified path to dataset files: /kaggle/input/ai-medleafx\n\nExploring dataset structure and counting images:\n\nFound 4 plant types: ['Sojina', 'Neem', 'Camphor', 'HariTaki']\n\nExploring 'Sojina': Found 3 status categories: ['Yellow Leaf', 'Healthy Leaf', 'Bacterial Spot']\n- Sojina_Yellow Leaf: 814 images\n- Sojina_Healthy Leaf: 860 images\n- Sojina_Bacterial Spot: 804 images\n\nExploring 'Neem': Found 4 status categories: ['Powdery Mildew', 'Yellow Leaf', 'Healthy Leaf', 'Shot Hole Leaf']\n- Neem_Powdery Mildew: 854 images\n- Neem_Yellow Leaf: 854 images\n- Neem_Healthy Leaf: 1021 images\n- Neem_Shot Hole Leaf: 834 images\n\nExploring 'Camphor': Found 3 status categories: ['Healthy Leaf', 'Shot Hole', 'Bacterial Spot']\n- Camphor_Healthy Leaf: 800 images\n- Camphor_Shot Hole: 795 images\n- Camphor_Bacterial Spot: 801 images\n\nExploring 'HariTaki': Found 3 status categories: ['Healthy Leaf', 'Shot Hole', 'Bacterial Spot']\n- HariTaki_Healthy Leaf: 816 images\n- HariTaki_Shot Hole: 802 images\n- HariTaki_Bacterial Spot: 803 images\n\nTotal number of classes: 13\n\nClass distribution:\n- Sojina_Yellow Leaf: 814\n- Sojina_Healthy Leaf: 860\n- Sojina_Bacterial Spot: 804\n- Neem_Powdery Mildew: 854\n- Neem_Yellow Leaf: 854\n- Neem_Healthy Leaf: 1021\n- Neem_Shot Hole Leaf: 834\n- Camphor_Healthy Leaf: 800\n- Camphor_Shot Hole: 795\n- Camphor_Bacterial Spot: 801\n- HariTaki_Healthy Leaf: 816\n- HariTaki_Shot Hole: 802\n- HariTaki_Bacterial Spot: 803\n\nTotal number of images: 10858\n\nDataset exploration and counting complete.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"seed = 42\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T05:25:56.473869Z","iopub.execute_input":"2025-10-27T05:25:56.474492Z","iopub.status.idle":"2025-10-27T05:25:56.479651Z","shell.execute_reply.started":"2025-10-27T05:25:56.474471Z","shell.execute_reply":"2025-10-27T05:25:56.478914Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ==============================================================================\n# CELL 3: DATA SPLITTING\n# ==============================================================================\nimport os, shutil\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Define paths\noriginal_base_dir = os.path.join(dataset_path, 'Resized Image')\nnew_base_dir = '/content/data'\n\n# Collect image paths and labels\ndata = []\nfor plant in os.listdir(original_base_dir):\n    for status in os.listdir(os.path.join(original_base_dir, plant)):\n        label = f\"{plant}_{status}\"\n        path = os.path.join(original_base_dir, plant, status)\n        for fname in os.listdir(path):\n            if fname.lower().endswith(('.jpg', '.jpeg', '.png','.JPG')):\n                data.append({'path': os.path.join(path, fname), 'label': label})\n\ndf = pd.DataFrame(data)\n\n# Split dataset: 80% train, 10% val, 10% test\ntrain_val, test = train_test_split(df, test_size=0.4, stratify=df['label'], random_state=42)\ntrain, val = train_test_split(train_val, test_size=0.1, stratify=train_val['label'], random_state=42)\n# Output summary\ntotal = len(df)\nprint(f\"\\n Dataset Split Summary:\")\nprint(f\"- Total images: {total}\")\nprint(f\"- Training set : {len(train):5d} images ({len(train)/total:.1%})\")\nprint(f\"- Validation set: {len(val):5d} images ({len(val)/total:.1%})\")\nprint(f\"- Test set     : {len(test):5d} images ({len(test)/total:.1%})\\n\")\n\n# Copy files\nfor split_df, split in zip([train, val, test], ['train', 'val', 'test']):\n    print(f\" Copying {split} files...\")\n    for _, row in split_df.iterrows():\n        dst_dir = os.path.join(new_base_dir, split, row['label'])\n        os.makedirs(dst_dir, exist_ok=True)\n        shutil.copy(row['path'], os.path.join(dst_dir, os.path.basename(row['path'])))\n    print(f\" {split.capitalize()} files copied: {len(split_df)}\")\n\nprint(\"\\n All files have been split and copied successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T05:26:10.210527Z","iopub.execute_input":"2025-10-27T05:26:10.211207Z","iopub.status.idle":"2025-10-27T05:34:47.988833Z","shell.execute_reply.started":"2025-10-27T05:26:10.211183Z","shell.execute_reply":"2025-10-27T05:34:47.988202Z"}},"outputs":[{"name":"stdout","text":"\n Dataset Split Summary:\n- Total images: 10858\n- Training set :  5862 images (54.0%)\n- Validation set:   652 images (6.0%)\n- Test set     :  4344 images (40.0%)\n\n Copying train files...\n Train files copied: 5862\n Copying val files...\n Val files copied: 652\n Copying test files...\n Test files copied: 4344\n\n All files have been split and copied successfully.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"IMG_SIZE = 224\n\ntrain_tfms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomApply([transforms.RandomRotation(degrees=10)], p=0.5),\n    transforms.RandomApply([transforms.ColorJitter(brightness=0.1, contrast=0.1)], p=0.3),\n    transforms.ToTensor(),\n    transforms.Lambda(lambda t: t.expand(3, -1, -1) if t.shape[0] == 1 else t),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])\n\nval_tfms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Lambda(lambda t: t.expand(3, -1, -1) if t.shape[0] == 1 else t),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])\n\n\n#  Create ImageFolder Datasets\ndata_dir = new_base_dir\ntrain_ds = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=train_tfms)\nval_ds   = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=val_tfms)\ntest_ds  = datasets.ImageFolder(root=os.path.join(data_dir, 'test'), transform=val_tfms)\n\n# STEP 7: Create DataLoaders\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader  = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n\n# STEP 8: Final Checks\nprint(\"\\n DataLoaders are ready.\")\nprint(f\"- Train classes : {train_ds.classes}\")\nprint(f\"- Num classes    : {len(train_ds.classes)}\")\nprint(f\"- Example image size: {IMG_SIZE}x{IMG_SIZE}\")\nprint(\" You can now start training!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T05:35:01.749969Z","iopub.execute_input":"2025-10-27T05:35:01.750853Z","iopub.status.idle":"2025-10-27T05:35:01.789790Z","shell.execute_reply.started":"2025-10-27T05:35:01.750816Z","shell.execute_reply":"2025-10-27T05:35:01.789125Z"}},"outputs":[{"name":"stdout","text":"\n DataLoaders are ready.\n- Train classes : ['Camphor_Bacterial Spot', 'Camphor_Healthy Leaf', 'Camphor_Shot Hole', 'HariTaki_Bacterial Spot', 'HariTaki_Healthy Leaf', 'HariTaki_Shot Hole', 'Neem_Healthy Leaf', 'Neem_Powdery Mildew', 'Neem_Shot Hole Leaf', 'Neem_Yellow Leaf', 'Sojina_Bacterial Spot', 'Sojina_Healthy Leaf', 'Sojina_Yellow Leaf']\n- Num classes    : 13\n- Example image size: 224x224\n You can now start training!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5: DEFINE THE DENSENET201 MODEL\n# ==============================================================================\nclass DenseNet201(nn.Module):\n    def __init__(self, num_classes, pretrained=True, freeze_backbone=True):\n        super().__init__()\n        \n        # 1. Load the DenseNet201 model and weights\n        weights = models.DenseNet201_Weights.DEFAULT if pretrained else None\n        self.densenet = models.densenet201(weights=weights)\n\n        if freeze_backbone:\n            # Freeze all parameters in the DenseNet backbone\n            for p in self.densenet.parameters():\n                p.requires_grad = False\n\n        # 2. Modify the final fully connected layer (DenseNet uses 'classifier')\n        # Get the input features of the existing classification layer\n        in_features = self.densenet.classifier.in_features\n        # Replace the layer with a new one for your specific number of classes\n        self.densenet.classifier = nn.Linear(in_features, num_classes)\n\n    def forward(self, x):\n        # Pass the input through the DenseNet model\n        return self.densenet(x)\n\nprint(\"DenseNet201 model class defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T05:35:05.397399Z","iopub.execute_input":"2025-10-27T05:35:05.397670Z","iopub.status.idle":"2025-10-27T05:35:05.404797Z","shell.execute_reply.started":"2025-10-27T05:35:05.397647Z","shell.execute_reply":"2025-10-27T05:35:05.404073Z"}},"outputs":[{"name":"stdout","text":"DenseNet201 model class defined.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ==============================================================================\n# CELL 6: TRAINING AND EVALUATION LOOP\n# ==============================================================================\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport time # Ensure this is imported in Cell 1\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# *** CHANGE 1: Instantiate DenseNet201 instead of ResNet50 ***\nmodel = DenseNet201(num_classes=len(train_ds.classes), pretrained=False, freeze_backbone=False).to(device)\n\nEPOCHS = 50\nbest_acc, best_path = 0.0, \"medical_plant_best.pth\"\ntrain_losses, train_accuracies = [], []\nval_losses, val_accuracies = [], []\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n# *** CHANGE 2: Update optimizer params to reference DenseNet's backbone (densenet) ***\noptimizer = optim.AdamW([\n    {'params': model.densenet.parameters(), 'lr': 1e-5}, \n], weight_decay=1e-4)\nscheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n\n# --- Training and Evaluation Functions (These remain the same) ---\ndef train_one_epoch():\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    for x,y in train_loader:\n        x,y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()*x.size(0)\n        pred = logits.argmax(1)\n        correct += (pred==y).sum().item()\n        total += x.size(0)\n    return running_loss/total, correct/total\n\n@torch.no_grad()\ndef evaluate(loader):\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n    for x,y in loader:\n        x,y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = criterion(logits, y)\n        running_loss += loss.item()*x.size(0)\n        pred = logits.argmax(1)\n        correct += (pred==y).sum().item()\n        total += x.size(0)\n    return running_loss/total, correct/total\n\n# --- Main Training Loop (This remains the same) ---\n\n# <<<--- START TRAINING TIMER ---<<<\ntraining_start_time = time.time()\n\nprint(f\"Starting training for {EPOCHS} epochs on {device}...\")\nfor epoch in range(1, EPOCHS+1):\n    tr_loss, tr_acc = train_one_epoch()\n    va_loss, va_acc = evaluate(val_loader)\n    scheduler.step()\n\n    # Log results\n    train_losses.append(tr_loss)\n    train_accuracies.append(tr_acc)\n    val_losses.append(va_loss)\n    val_accuracies.append(va_acc)\n\n    # Save the best model\n    if va_acc > best_acc:\n        best_acc = va_acc\n        torch.save(model.state_dict(), best_path)\n        print(f\"✓ Saved new best model to {best_path} (val acc {best_acc:.4f})\")\n\n    print(f\"Epoch {epoch:02d} | Train Loss {tr_loss:.4f} Acc {tr_acc:.4f} | Val Loss {va_loss:.4f} Acc {va_acc:.4f}\")\n\n# >>>--- END TRAINING TIMER --->>>\ntraining_end_time = time.time()\ntraining_duration = training_end_time - training_start_time\n\nprint(f\"\\nFinished Training. Best Validation Accuracy: {best_acc:.4f}\")\nprint(f\"Total Training Time: {training_duration:.2f} seconds ({training_duration/60:.2f} minutes)\")\n\n# --- Final Test Evaluation ---\n\n# Load the best model weights\nmodel.load_state_dict(torch.load(best_path))\n\ntest_start_time = time.time()\ntest_loss, test_acc = evaluate(test_loader)\ntest_end_time = time.time()\ntest_duration = test_end_time - test_start_time\n\nprint(f\"\\nPerforming final evaluation on the unseen test set...\")\nprint(f\"FINAL TEST RESULTS -> Loss: {test_loss:.4f} | Accuracy: {test_acc:.4f}\")\nprint(f\"Testing Time: {test_duration:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T05:35:20.598005Z","iopub.execute_input":"2025-10-27T05:35:20.598239Z","iopub.status.idle":"2025-10-27T06:59:33.227760Z","shell.execute_reply.started":"2025-10-27T05:35:20.598224Z","shell.execute_reply":"2025-10-27T06:59:33.226860Z"}},"outputs":[{"name":"stdout","text":"Starting training for 50 epochs on cuda...\n✓ Saved new best model to medical_plant_best.pth (val acc 0.4540)\nEpoch 01 | Train Loss 2.2378 Acc 0.3170 | Val Loss 1.8833 Acc 0.4540\n✓ Saved new best model to medical_plant_best.pth (val acc 0.5706)\nEpoch 02 | Train Loss 1.8163 Acc 0.4879 | Val Loss 1.5662 Acc 0.5706\n✓ Saved new best model to medical_plant_best.pth (val acc 0.6488)\nEpoch 03 | Train Loss 1.5708 Acc 0.5751 | Val Loss 1.3509 Acc 0.6488\n✓ Saved new best model to medical_plant_best.pth (val acc 0.7285)\nEpoch 04 | Train Loss 1.3750 Acc 0.6546 | Val Loss 1.1924 Acc 0.7285\n✓ Saved new best model to medical_plant_best.pth (val acc 0.7853)\nEpoch 05 | Train Loss 1.2359 Acc 0.7223 | Val Loss 1.1003 Acc 0.7853\n✓ Saved new best model to medical_plant_best.pth (val acc 0.8021)\nEpoch 06 | Train Loss 1.1529 Acc 0.7579 | Val Loss 1.0375 Acc 0.8021\n✓ Saved new best model to medical_plant_best.pth (val acc 0.8206)\nEpoch 07 | Train Loss 1.0828 Acc 0.7863 | Val Loss 0.9657 Acc 0.8206\n✓ Saved new best model to medical_plant_best.pth (val acc 0.8328)\nEpoch 08 | Train Loss 1.0327 Acc 0.8001 | Val Loss 0.9293 Acc 0.8328\nEpoch 09 | Train Loss 0.9896 Acc 0.8205 | Val Loss 0.9478 Acc 0.8206\n✓ Saved new best model to medical_plant_best.pth (val acc 0.8359)\nEpoch 10 | Train Loss 0.9536 Acc 0.8425 | Val Loss 0.9325 Acc 0.8359\n✓ Saved new best model to medical_plant_best.pth (val acc 0.8558)\nEpoch 11 | Train Loss 0.9296 Acc 0.8531 | Val Loss 0.8858 Acc 0.8558\n✓ Saved new best model to medical_plant_best.pth (val acc 0.8804)\nEpoch 12 | Train Loss 0.9127 Acc 0.8518 | Val Loss 0.8454 Acc 0.8804\nEpoch 13 | Train Loss 0.8864 Acc 0.8731 | Val Loss 0.8874 Acc 0.8604\n✓ Saved new best model to medical_plant_best.pth (val acc 0.8865)\nEpoch 14 | Train Loss 0.8696 Acc 0.8762 | Val Loss 0.8263 Acc 0.8865\nEpoch 15 | Train Loss 0.8480 Acc 0.8872 | Val Loss 0.8543 Acc 0.8758\nEpoch 16 | Train Loss 0.8394 Acc 0.8936 | Val Loss 0.8306 Acc 0.8850\n✓ Saved new best model to medical_plant_best.pth (val acc 0.9172)\nEpoch 17 | Train Loss 0.8208 Acc 0.9000 | Val Loss 0.7908 Acc 0.9172\nEpoch 18 | Train Loss 0.7994 Acc 0.9070 | Val Loss 0.8051 Acc 0.8926\nEpoch 19 | Train Loss 0.7990 Acc 0.9101 | Val Loss 0.7978 Acc 0.9095\nEpoch 20 | Train Loss 0.7863 Acc 0.9157 | Val Loss 0.7930 Acc 0.9049\nEpoch 21 | Train Loss 0.7728 Acc 0.9190 | Val Loss 0.7731 Acc 0.9156\nEpoch 22 | Train Loss 0.7579 Acc 0.9324 | Val Loss 0.7757 Acc 0.9095\n✓ Saved new best model to medical_plant_best.pth (val acc 0.9202)\nEpoch 23 | Train Loss 0.7574 Acc 0.9270 | Val Loss 0.7729 Acc 0.9202\n✓ Saved new best model to medical_plant_best.pth (val acc 0.9233)\nEpoch 24 | Train Loss 0.7479 Acc 0.9324 | Val Loss 0.7671 Acc 0.9233\nEpoch 25 | Train Loss 0.7434 Acc 0.9343 | Val Loss 0.7706 Acc 0.9172\nEpoch 26 | Train Loss 0.7363 Acc 0.9347 | Val Loss 0.7711 Acc 0.9156\nEpoch 27 | Train Loss 0.7287 Acc 0.9400 | Val Loss 0.7624 Acc 0.9187\n✓ Saved new best model to medical_plant_best.pth (val acc 0.9294)\nEpoch 28 | Train Loss 0.7324 Acc 0.9360 | Val Loss 0.7516 Acc 0.9294\nEpoch 29 | Train Loss 0.7214 Acc 0.9439 | Val Loss 0.7569 Acc 0.9187\nEpoch 30 | Train Loss 0.7102 Acc 0.9505 | Val Loss 0.7696 Acc 0.9141\nEpoch 31 | Train Loss 0.7129 Acc 0.9447 | Val Loss 0.7525 Acc 0.9187\nEpoch 32 | Train Loss 0.7055 Acc 0.9527 | Val Loss 0.7667 Acc 0.9187\nEpoch 33 | Train Loss 0.6989 Acc 0.9539 | Val Loss 0.7558 Acc 0.9187\n✓ Saved new best model to medical_plant_best.pth (val acc 0.9340)\nEpoch 34 | Train Loss 0.7049 Acc 0.9507 | Val Loss 0.7569 Acc 0.9340\nEpoch 35 | Train Loss 0.6991 Acc 0.9521 | Val Loss 0.7497 Acc 0.9264\nEpoch 36 | Train Loss 0.6959 Acc 0.9560 | Val Loss 0.7465 Acc 0.9202\nEpoch 37 | Train Loss 0.6912 Acc 0.9565 | Val Loss 0.7456 Acc 0.9264\nEpoch 38 | Train Loss 0.6859 Acc 0.9589 | Val Loss 0.7448 Acc 0.9264\nEpoch 39 | Train Loss 0.6900 Acc 0.9597 | Val Loss 0.7443 Acc 0.9264\nEpoch 40 | Train Loss 0.6820 Acc 0.9625 | Val Loss 0.7494 Acc 0.9218\nEpoch 41 | Train Loss 0.6868 Acc 0.9611 | Val Loss 0.7436 Acc 0.9310\nEpoch 42 | Train Loss 0.6862 Acc 0.9614 | Val Loss 0.7472 Acc 0.9294\nEpoch 43 | Train Loss 0.6866 Acc 0.9592 | Val Loss 0.7509 Acc 0.9248\nEpoch 44 | Train Loss 0.6824 Acc 0.9606 | Val Loss 0.7471 Acc 0.9264\nEpoch 45 | Train Loss 0.6753 Acc 0.9614 | Val Loss 0.7513 Acc 0.9279\nEpoch 46 | Train Loss 0.6796 Acc 0.9633 | Val Loss 0.7491 Acc 0.9294\nEpoch 47 | Train Loss 0.6856 Acc 0.9591 | Val Loss 0.7456 Acc 0.9279\nEpoch 48 | Train Loss 0.6741 Acc 0.9637 | Val Loss 0.7662 Acc 0.9110\nEpoch 49 | Train Loss 0.6754 Acc 0.9654 | Val Loss 0.7490 Acc 0.9264\nEpoch 50 | Train Loss 0.6800 Acc 0.9594 | Val Loss 0.7447 Acc 0.9248\n\nFinished Training. Best Validation Accuracy: 0.9340\nTotal Training Time: 5028.87 seconds (83.81 minutes)\n\nPerforming final evaluation on the unseen test set...\nFINAL TEST RESULTS -> Loss: 0.7681 | Accuracy: 0.9169\nTesting Time: 22.63 seconds\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}